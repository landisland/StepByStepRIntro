# This R files contain step by step R tutorial from ISLR2 (https://hastie.su.domains/ISLR2/ISLRv2_website.pdf)
# And From ISLR2 Chapter 3.6
# Author: landisland

# import library
library(MASS)
library(ISLR2)

################## Simple Linear Regression ####################
# head() function can check first few lines of our data
# Boston is a dataset in ISLR2
head(Boston)

# linear model lm(y~x, data = table_name)
lm(medv ~ lstat, data = Boston)

# or use attach() to attach table
attach(Boston)
lm(medv ~ lstat)

# store linear model generated by lm() function
model <- lm(medv ~ lstat)

# summary() function can give us detailed information about our model
summary(model)

# we can use names() function to find out what information are stored in model
names(model)

# we can extract information by name
model$coefficients

# use confint() function we can get confidence interval for the 
#   coefficient estimates
confint(model)

# predict() function can be used to produce confidence intervals
#   and prediction intervals for the prediction of `medv` for a given
#     value of `lstat`
predict(model, data.frame(lstat = (c(5, 10, 15))), interval = "confidence")

predict(model, data.frame(lstat = (c(5, 10, 15))), interval = "prediction")

# plot() function
plot(lstat, medv)

# abline() function, lwd param means line width, col param means color
abline(model, lwd = 1, col = "red")

# par() function and mfrow() function can tell R to 
#   split the display screen into seprate panels so that
#     multiple plots can be viewed simultaneously
par(mfrow = c(2, 2))
plot(model)

# we can use residuals() function, this plot is the same as the top left plot
#   shown in line 49
plot(predict(model), residuals(model))

# rstudent() function can return studentized residuals
plot(predict(model), rstudent(model))

# hatvalues() can compute leverage statistics
plot(hatvalues((model)))

## which.max() function returns the index of the largest element of a vector
##  in this case, observation 375 has the largest value
which.max(hatvalues((model))) 

################## Simple Linear Regression ####################


################## Multiple Linear Regression ####################

# now we can have several predictors in our model
model_multiple <- lm(medv ~ lstat + age, data = Boston)

# get detailed information
summary(model_multiple)

# Boston dataset contains 12 variables, we can use a tricky way to contain
#   all these variables without typing them one by one
model_all_variable <- lm(medv ~ ., data = Boston)

summary(model_all_variable)

# compute variance inflation factors to check whether our variables are
#   collinear or not (collinearity), we need to use vif() function, which is 
#     included in car library

#install car library
library(car)

# vif() function
# when vif = 1, there is no collinearity
# when vif > 5, there is serious collinearity
vif(model_all_variable)

# what if we want to perform a regression using all of the variables BUT ONE?
model_exclude_one <- lm(medv ~ . - age, data = Boston)
# or you can update existing model
model_exclude_one <- update(model_all_variable, ~ . - age)

# interaction terms
# say we have model with predictors `lstat` and `age`, and now we want to add
#   add interaction term
model_multiple <- lm(medv ~ lstat + age, data = Boston)

# add interaction term
# there are 2 ways:
#   1. use syntax lstat:age, medv ~ lstat + age + lstat:age
#   2. use syntax lstat*age, medv ~ lstat * age (this simultaneously includes
#     lstat, age, and the interaction term lstatXage as predictors)
model_multiple_with_interaction <- lm(medv ~ lstat + age + lstat:age, data = Boston)

summary(model_multiple_with_interaction)

################## Multiple Linear Regression ####################

######## Use lm() function to accommodate non-linear transformations ###########
model_square <- lm(medv ~ lstat + I(lstat^2))

summary(model_square)
# from summary data we can find out the quadratic term leads to an improved model

# use anova() function to further quantify the extent to which the quadratic fit
#   is superior to the linear fit
anova(model,model_square)

# the anova result's p-value is virtually zero, which means quadratic model is 
#   far superior to the model that only contains the predictor `lstat`

# and we can create cubic fit with adding I(X^3). There is a better way for
#   higher order polynomials by using poly() function
model_poly <- lm(medv ~ poly(lstat, 5))
summary(model_poly)
# This suggests that including additional polynomial terms, up to ﬁfth order, 
#   leads to an improvement in the model ﬁt! However, further investigation of 
#     the data reveals that no polynomial terms beyond ﬁfth order have 
#       signiﬁcant p-values in a regression ﬁt.

######## Use lm() function to accommodate non-linear transformations ###########

################## Qualitative Predictors ####################
# now we examine the Carseats data, which is part of the ISLR2 library
# we will attempt to predict `Sales` in 400 locations based on a number of predictors

# in this dataset, we have predictors like `Shelveloc` takes on three possible
#   values: bad, medium, and good. (which is qualitative)
# For these qualitative variable, R generates dummy variables automatically.

model_qualitative <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
summary(model_qualitative)

# contrasts() function returns the coding that R uses for the dummy variables
attach(Carseats)
contrasts(ShelveLoc)

################## Qualitative Predictors ####################

################## Writing Functions ####################
# we now try to create our own function `LoadLibraries()`

LoadLibraries <- function(){
  library(ISLR2)
  library(MASS)
  print("The libraries have been loaded!")
}
################## Writing Functions ####################